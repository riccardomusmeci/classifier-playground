###### Datamodule ######
datamodule:
  class_map: {                              # your class map 
    0: "class_a",                           # you can also aggregate classes { 0: 'class_a', 1: ['class_b', 'class_c']}
    1: "class_b"                            # based on your dataset's folders
  }
  imbalanced: true                          # if dataset is imblanced, set this to true
  max_samples_per_class: null               # maximum number of annotations per class
  random_sample: false                      # true if you wanto to select your max annotations randomly
  batch_size: 128                           # batch size
  shuffle: true                             # if imbalanced is true, this is ignored
  num_workers: 5                            # num data loader workers
  pin_memory: true        
  drop_last: false
  persistent_workers: true

###### Trainer ######
trainer:
  accelerator: gpu                           # if gpu, pytorch lightning will detect which one (also MPS for M1)                                 
  devices: 1                                
  max_epochs: 100       
  precision: 16                              # leave it to 16 for faster trainings
  check_val_every_n_epoch: 1
  gradient_clip_val: null

###### Model ######
model:
  model_name: resnet18                        # timm model (check out here https://github.com/rwightman/pytorch-image-models/blob/master/results/results-imagenet.csv)
  pretrained: true                            # pretrained weights

###### Loss ###### 
loss:                                           
  name: xent                                  # name of the loss criterion to use
  weight: [0.55544285, 5.00914808]            # weight for each class in the loss computation (if dataset is imbalanced) 
  label_smoothing: 0.1                        # label smoothing factor -> to prevent model being overconfident (calibration)
  
###### Optimizer ######  
optimizer:
  name: sam                                   # optimization algorithm (sgd, adam, adamw)
  base_optimer: sgd                           # base optimizer per sam
  rho: 2                                      # Rho parameter for SAM
  lr: 0.1                                     # base learning rate at the start of the training
  adaptive: true                              # True if you want to use the Adaptive SAM.
  weight_decay: 0.0005                        # l2 weigth decay
  momentum: 0.9                               # sgd momentum

###### LR Scheduler ######
lr_scheduler:
  name: cosine_restarts                       # lr_scheduler name
  T_0: 5                                      # when to start cosine scheduling
  T_mult: 2                                   # multiplicative factor between restarting
  eta_min: 0                                  # min lr to reach
  
transform: 
  img_size: 224                               # input image size
  crop_resize_p: 0.5                          # crop and resize probabilty  
  mean: [0.485, 0.456, 0.406]                 # ImageNet mean normalization ([0.485, 0.456, 0.406])
  std: [0.229, 0.224, 0.225]                  # ImageNet std normalization ([0.229, 0.224, 0.225])
  brightness: 0.4                             # color jitter brightness
  contrast: 0.4                               # color jitter contrast
  saturation: 0.2                             # color jitter saturation
  hue: 0.1                                    # color jitter hue            
  color_jitter_p: 0.5                         # color jitter transformation probability
  grayscale_p: 0.2                            # grayscale transformation probabilty
  h_flip_p: 0.5                               # horizontal flip transformation probabilty
  kernel: [5, 5]                              # gaussian blur kernel size
  sigma: [.1, 2]                              # gaussian blur params
  solarization_p: 0.2                         # solarization probability
  solarize_t: 170                             # solarization threshold


##### Callbacks ##### (the less you edit it the better :D)
callbacks:
  filename: epoch={epoch}-step={step}-val_loss={loss/val:.3f}-val_acc={acc/val:.3f}-val_f1={f1/val:.3f}-cal_err={cal_err/val:.5f}
  monitor: f1/val
  mode: max
  save_top_k: 10
  patience: 10